# this corresponds to the default config
train:
  dataset_path: ImageNet:split=TRAIN
  batch_size_per_gpu: 64
  OFFICIAL_EPOCH_LENGTH: 1250
  num_workers: 7
evaluation:
  eval_period_iterations: 100000  # save additional teacher checkpoint
compute_precision:
  teacher:
    backbone:
      sharding_strategy: NO_SHARD  # NO_SHARD or SHARD_GRAD_OP or FULL_SHARD
      mixed_precision:
        param_dtype: fp32
        reduce_dtype: fp32
    dino_head:
      sharding_strategy: NO_SHARD
    ibot_head:
      sharding_strategy: NO_SHARD
  student:
    backbone:
      sharding_strategy: NO_SHARD
      mixed_precision:
        param_dtype: fp32
        reduce_dtype: fp32
    dino_head:
      sharding_strategy: NO_SHARD
    ibot_head:
      sharding_strategy: NO_SHARD
teacher:
  warmup_teacher_temp_epochs: 30
student:
  arch: nextvit  # vit_large, nextvit
  block_chunks: 4
optim:
  base_lr: 0.0002
  epochs: 100
  warmup_epochs: 10